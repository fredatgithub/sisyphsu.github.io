<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"><link rel="stylesheet" href="/css/main.css?v=7.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png?v=7.0.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=7.0.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=7.0.0"><link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222"><script id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"7.0.0",sidebar:{position:"left",width:300,display:"always",offset:12,b2t:!0,scrollpercent:!0,onmobile:!1,dimmer:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta name="description" content="生产环境中，Flume一般会10分钟归并一个日志文件。每个日志文件都只有10MB~20MB，压缩后页只有2~5MB。Hive面对这样的小文件，该怎么进行性能优化呢？Job预处理Hive执行每条SQL前，都会根据SQL查询范围来统计需要扫描的分区，之后再遍历这些分区来收集全部数据文件，这些操作都是基于NameNode进行的，这个过程中有两点需要注意：尽可能小的明确指定分区范围，一般都采用日期分区，那"><meta name="keywords" content="Hive"><meta property="og:type" content="article"><meta property="og:title" content="Hive性能优化之Combine小文件"><meta property="og:url" content="https://sulin.me/2015/2CW1CQ8.html"><meta property="og:site_name" content="小木屋"><meta property="og:description" content="生产环境中，Flume一般会10分钟归并一个日志文件。每个日志文件都只有10MB~20MB，压缩后页只有2~5MB。Hive面对这样的小文件，该怎么进行性能优化呢？Job预处理Hive执行每条SQL前，都会根据SQL查询范围来统计需要扫描的分区，之后再遍历这些分区来收集全部数据文件，这些操作都是基于NameNode进行的，这个过程中有两点需要注意：尽可能小的明确指定分区范围，一般都采用日期分区，那"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2019-03-04T11:51:13.693Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Hive性能优化之Combine小文件"><meta name="twitter:description" content="生产环境中，Flume一般会10分钟归并一个日志文件。每个日志文件都只有10MB~20MB，压缩后页只有2~5MB。Hive面对这样的小文件，该怎么进行性能优化呢？Job预处理Hive执行每条SQL前，都会根据SQL查询范围来统计需要扫描的分区，之后再遍历这些分区来收集全部数据文件，这些操作都是基于NameNode进行的，这个过程中有两点需要注意：尽可能小的明确指定分区范围，一般都采用日期分区，那"><link rel="canonical" href="https://sulin.me/2015/2CW1CQ8.html"><script id="page.configurations">CONFIG.page={sidebar:""}</script><title>Hive性能优化之Combine小文件 | 小木屋</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">小木屋</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">苏林的个人博客</h1></div><div class="site-nav-toggle"><button aria-label="切换导航栏"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签<span class="badge">39</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类<span class="badge">13</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档<span class="badge">49</span></a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>站点地图</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://sulin.me/2015/2CW1CQ8.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="sulin"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="小木屋"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">Hive性能优化之Combine小文件</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2015-03-30 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-30T00:00:00+08:00">2015-03-30</time> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2019-03-04 19:51:13" itemprop="dateModified" datetime="2019-03-04T19:51:13+08:00">2019-03-04</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/大数据/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>生产环境中，Flume一般会10分钟归并一个日志文件。每个日志文件都只有10MB~20MB，压缩后页只有2~5MB。Hive面对这样的小文件，该怎么进行性能优化呢？</p><h1 id="Job预处理"><a href="#Job预处理" class="headerlink" title="Job预处理"></a>Job预处理</h1><p>Hive执行每条SQL前，都会根据SQL查询范围来统计需要扫描的分区，之后再遍历这些分区来收集全部数据文件，这些操作都是基于NameNode进行的，<br>这个过程中有两点需要注意：</p><ul><li>尽可能小的明确指定分区范围，一般都采用日期分区，那么在查询条件中就要尽可能精确的限定日期查询范围。</li><li>Job预处理结束后，整个MapReduce的输入文件就已经确定了。此时如果删除被Job收集到的文件的话，就会导致MapReduce执行失败，而新增加的文件也不会被Job覆盖到。</li></ul><h1 id="Map数量优化"><a href="#Map数量优化" class="headerlink" title="Map数量优化"></a>Map数量优化</h1><p>Flume生成的日志小又多，生产环境每个月能新增超过1w个日志文件。如此庞大的日志文件数量，会导致Job中map数量超大而计算速度极慢。</p><p>可以通过配置，让Hive采用<em>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</em>将小文件合并起来，这样一来map数量就大大减少，job耗时大幅提升。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.max.split.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>128000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.min.split.size.per.node<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>64000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.min.split.size.per.rack<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>64000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hadoop.supports.splittable.combineinputformat<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><em>hive.hadoop.supports.splittable.combineinputformat</em>配置很重要，因为Hive默认不会合并compression小文件，具体原因见如下Hive源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Since there is no easy way of knowing whether MAPREDUCE-1597 is present in the tree or not,</span></span><br><span class="line"><span class="comment">// we use a configuration variable for the same</span></span><br><span class="line"><span class="comment">// 如果hive.hadoop.supports.splittable.combineinputformat为false的话，就会进入下面的if</span></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>.mrwork != <span class="keyword">null</span> &amp;&amp; !<span class="keyword">this</span>.mrwork.getHadoopSupportsSplittable()) &#123;</span><br><span class="line">    <span class="comment">// The following code should be removed, once</span></span><br><span class="line">    <span class="comment">// https://issues.apache.org/jira/browse/MAPREDUCE-1597 is fixed.</span></span><br><span class="line">    <span class="comment">// Hadoop does not handle non-splittable files correctly for CombineFileInputFormat,</span></span><br><span class="line">    <span class="comment">// so don't use CombineFileInputFormat for non-splittable files</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//ie, dont't combine if inputformat is a TextInputFormat and has compression turned on</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (inputFormat <span class="keyword">instanceof</span> TextInputFormat) &#123;</span><br><span class="line">        Queue&lt;Path&gt; dirs = <span class="keyword">new</span> LinkedList&lt;Path&gt;();</span><br><span class="line">        FileStatus fStats = inpFs.getFileStatus(path);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If path is a directory</span></span><br><span class="line">        <span class="keyword">if</span> (fStats.isDir()) &#123;</span><br><span class="line">            dirs.offer(path);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((<span class="keyword">new</span> CompressionCodecFactory(job)).getCodec(path) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//if compresssion codec is set, use HiveInputFormat.getSplits (don't combine)</span></span><br><span class="line">            <span class="comment">// 遇到压缩codec，直接使用HiveInputFormat而不combine</span></span><br><span class="line">            splits = <span class="keyword">super</span>.getSplits(job, numSplits);</span><br><span class="line">            <span class="keyword">return</span> splits;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (dirs.peek() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            Path tstPath = dirs.remove();</span><br><span class="line">            FileStatus[] fStatus = inpFs.listStatus(tstPath, FileUtils.HIDDEN_FILES_PATH_FILTER);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> idx = <span class="number">0</span>; idx &lt; fStatus.length; idx++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (fStatus[idx].isDir()) &#123;</span><br><span class="line">                    dirs.offer(fStatus[idx].getPath());</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((<span class="keyword">new</span> CompressionCodecFactory(job)).getCodec(</span><br><span class="line">                        fStatus[idx].getPath()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">//if compresssion codec is set, use HiveInputFormat.getSplits (don't combine)</span></span><br><span class="line">                    <span class="comment">// 遇到压缩codec，直接使用HiveInputFormat而不combine</span></span><br><span class="line">                    splits = <span class="keyword">super</span>.getSplits(job, numSplits);</span><br><span class="line">                    <span class="keyword">return</span> splits;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>必须配置hive.hadoop.supports.splittable.combineinputformat为true，才会让Hive合并compression小文件。</p></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>sulin</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://sulin.me/2015/2CW1CQ8.html" title="Hive性能优化之Combine小文件">https://sulin.me/2015/2CW1CQ8.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Hive/" rel="tag"># Hive</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2015/2311KVV.html" rel="next" title="Flume日常使用问题汇总与解决办法"><i class="fa fa-chevron-left"></i> Flume日常使用问题汇总与解决办法</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2015/3V5KNGA.html" rel="prev" title="Hive故障排查备忘录">Hive故障排查备忘录 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="sulin"><p class="site-author-name" itemprop="name">sulin</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">49</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">39</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="mailto:sulinixl@gmail.com" title="E-Mail &rarr; mailto:sulinixl@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a> </span><span class="links-of-author-item"><a href="https://github.com/sisyphsu" title="GitHub &rarr; https://github.com/sisyphsu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a> </span><span class="links-of-author-item"><a href="https://twitter.com/sulinixl" title="Twitter &rarr; https://twitter.com/sulinixl" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i></a> </span><span class="links-of-author-item"><a href="https://stackoverflow.com/users/5932976/sulin" title="StackOverflow &rarr; https://stackoverflow.com/users/5932976/sulin" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a></span></div></div></div><div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Job预处理"><span class="nav-number">1.</span> <span class="nav-text">Job预处理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Map数量优化"><span class="nav-number">2.</span> <span class="nav-text">Map数量优化</span></a></li></ol></div></div></div><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span> <span class="with-love" id="animate"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">sulin</span></div><div style="display:none"><script src="//s96.cnzz.com/z_stat.php?id=1276400179&web_id=1276400179"></script></div></div></footer></div><script>"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script src="/lib/jquery/index.js?v=2.1.3"></script><script src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script src="/js/src/utils.js?v=7.0.0"></script><script src="/js/src/motion.js?v=7.0.0"></script><script src="/js/src/affix.js?v=7.0.0"></script><script src="/js/src/schemes/pisces.js?v=7.0.0"></script><script src="/js/src/scrollspy.js?v=7.0.0"></script><script src="/js/src/post-details.js?v=7.0.0"></script><script src="/js/src/bootstrap.js?v=7.0.0"></script><script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script><script>var gitalk=new Gitalk({clientID:"e392da7bb9c29fcbab05",clientSecret:"379a583593484a9958b32f40ef62f7ffa7a36059",repo:"sisyphsu.github.io",owner:"sisyphsu",admin:["sisyphsu"],id:md5(location.pathname),distractionFreeMode:"true"});gitalk.render("gitalk-container")</script></body></html>